{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shayshankr/Machine-Learning-Project/blob/main/Machine_learning_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Importing Necessary Libraries"
      ],
      "metadata": {
        "id": "LMVzA98xvdM1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IIZ5astsvLIt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, roc_curve, accuracy_score,\n",
        "    classification_report, confusion_matrix, precision_recall_curve\n",
        ")\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from tensorflow import keras\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Load Dataset"
      ],
      "metadata": {
        "id": "b_4wPShcvkz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/bank_transactions_data_2.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "print(\"Dataset Loaded Successfully!\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lWWI_V1vmDO",
        "outputId": "d3b3542a-b15e-42b4-a4cc-05af89172598"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Loaded Successfully!\n",
            "  TransactionID AccountID  TransactionAmount      TransactionDate  \\\n",
            "0      TX000001   AC00128              14.09  2023-04-11 16:29:14   \n",
            "1      TX000002   AC00455             376.24  2023-06-27 16:44:19   \n",
            "2      TX000003   AC00019             126.29  2023-07-10 18:16:08   \n",
            "3      TX000004   AC00070             184.50  2023-05-05 16:32:11   \n",
            "4      TX000005   AC00411              13.45  2023-10-16 17:51:24   \n",
            "\n",
            "  TransactionType   Location DeviceID      IP Address MerchantID Channel  \\\n",
            "0           Debit  San Diego  D000380  162.198.218.92       M015     ATM   \n",
            "1           Debit    Houston  D000051     13.149.61.4       M052     ATM   \n",
            "2           Debit       Mesa  D000235  215.97.143.157       M009  Online   \n",
            "3           Debit    Raleigh  D000187  200.13.225.150       M002  Online   \n",
            "4          Credit    Atlanta  D000308    65.164.3.100       M091  Online   \n",
            "\n",
            "   CustomerAge CustomerOccupation  TransactionDuration  LoginAttempts  \\\n",
            "0           70             Doctor                   81              1   \n",
            "1           68             Doctor                  141              1   \n",
            "2           19            Student                   56              1   \n",
            "3           26            Student                   25              1   \n",
            "4           26            Student                  198              1   \n",
            "\n",
            "   AccountBalance PreviousTransactionDate  \n",
            "0         5112.21     2024-11-04 08:08:08  \n",
            "1        13758.91     2024-11-04 08:09:35  \n",
            "2         1122.35     2024-11-04 08:07:04  \n",
            "3         8569.06     2024-11-04 08:09:06  \n",
            "4         7429.40     2024-11-04 08:06:39  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Exploratory Data Analysis (EDA)"
      ],
      "metadata": {
        "id": "WPFh7Y9pvq6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.info())\n"
      ],
      "metadata": {
        "id": "Gl8carXAvuEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Missing Values:\\n\", df.isnull().sum())"
      ],
      "metadata": {
        "id": "Z9GXuuJnvy7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Target distribution"
      ],
      "metadata": {
        "id": "emCyA5EHv3x0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Target distribution\n",
        "sns.countplot(x='TransactionType', data=df)\n",
        "plt.title(\"Distribution of Transaction Types\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zJ-HHq99v52m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kgSodSQ-0J76"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Correlation Matrix"
      ],
      "metadata": {
        "id": "WqLLojLQv_Ch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Matrix\n",
        "# Convert TransactionDate to datetime if it's a string\n",
        "\n",
        "if 'TransactionDate' in df.columns and df['TransactionDate'].dtype == 'object':\n",
        "    df['TransactionDate'] = pd.to_datetime(df['TransactionDate'])\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(df.select_dtypes(include='number').corr(), annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Matrix Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UC5rnxaSwAUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Histograms and Boxplots"
      ],
      "metadata": {
        "id": "Ne_gWkJUwETA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize distribution and spread of numerical features\n",
        "numerical_features = ['TransactionAmount', 'CustomerID']\n",
        "for col in numerical_features:\n",
        "    if col in df.columns:\n",
        "        sns.histplot(df[col], kde=True)\n",
        "        plt.title(f'Distribution of {col}')\n",
        "        plt.show()\n",
        "        sns.boxplot(y=df[col])\n",
        "        plt.title(f'Box Plot of {col}')\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "9M4GiezFwHvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Data Preprocessing & Feature Engineering"
      ],
      "metadata": {
        "id": "v16n8dtuwPT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for duplicate rows\n",
        "duplicate_rows = df[df.duplicated()]\n",
        "print(f\"Number of duplicate rows: {len(duplicate_rows)}\")\n"
      ],
      "metadata": {
        "id": "4RbV4UhJwUUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature engineering: convert date to timestamp and drop original\n",
        "df.drop_duplicates(inplace=True)\n",
        "if 'TransactionDate' in df.columns:\n",
        "    df['TransactionDate'] = pd.to_datetime(df['TransactionDate'])\n",
        "    df['TransactionTimestamp'] = df['TransactionDate'].astype(int) / 10**9\n",
        "    df.drop(columns=['TransactionDate'], inplace=True)"
      ],
      "metadata": {
        "id": "KiInZvRPwQZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Log Transformation"
      ],
      "metadata": {
        "id": "8fFSW-UBwiL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply log transformation to reduce skewness\n",
        "if 'TransactionAmount' in df.columns:\n",
        "    df['TransactionAmount_log'] = np.log1p(df['TransactionAmount'])"
      ],
      "metadata": {
        "id": "D107Kg3AwjBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoding"
      ],
      "metadata": {
        "id": "OFiK3L0pwnNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode categorical columns using LabelEncoder\n",
        "label_enc = LabelEncoder()\n",
        "categorical_cols = ['TransactionType', 'Channel', 'CustomerOccupation', 'Location']\n",
        "for col in categorical_cols:\n",
        "    if col in df.columns:\n",
        "        df[col] = label_enc.fit_transform(df[col].astype(str))"
      ],
      "metadata": {
        "id": "7QvYPt2IwoUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Drop unnecessary columns\n"
      ],
      "metadata": {
        "id": "Orqr9ulHwtTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop irrelevant or ID-based columns\n",
        "df.drop(columns=['TransactionID', 'AccountID', 'MerchantID', 'DeviceID', 'IP Address'], inplace=True, errors='ignore')\n",
        "\n",
        "# Keep only numeric features\n",
        "df = df.select_dtypes(include=['number'])"
      ],
      "metadata": {
        "id": "yfyy1c5hwwaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Train-Test Split and SMOTE"
      ],
      "metadata": {
        "id": "58J9Hn92w2Xy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation Strategy:\n",
        "\n",
        "80/20 stratified train-test split to preserve class balance.\n",
        "\n",
        " XGBoost will also use 3-fold cross-validation in tuning."
      ],
      "metadata": {
        "id": "Nv8VRfrj1fL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features and target\n",
        "X = df.drop(columns=['TransactionType'])\n",
        "y = df['TransactionType']"
      ],
      "metadata": {
        "id": "HAeBz0za1tJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply SMOTE to balance class distribution\n",
        "smote = SMOTE(random_state=42)\n",
        "X, y = smote.fit_resample(X, y)"
      ],
      "metadata": {
        "id": "101YjRrm1wNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "ELdRAkqS1046"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "neFbaj-u14Vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Model Training: Random Forest & XGBoost"
      ],
      "metadata": {
        "id": "A-mqfVByxAvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup XGBoost with a parameter grid for tuning\n",
        "xgb_model = XGBClassifier()\n",
        "param_grid_xgb = {\n",
        "    'n_estimators': [100],\n",
        "    'learning_rate': [0.1],\n",
        "    'max_depth': [5]\n",
        "}\n"
      ],
      "metadata": {
        "id": "Nwy9mbSm2jxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use 3-fold cross-validation during GridSearchCV for XGBoost\n",
        "grid_search_xgb = GridSearchCV(\n",
        "    xgb_model, param_grid_xgb, scoring='roc_auc', cv=3, n_jobs=-1\n",
        ")\n",
        "grid_search_xgb.fit(X_train, y_train)\n",
        "best_xgb = grid_search_xgb.best_estimator_"
      ],
      "metadata": {
        "id": "mNiSa9Ba2m1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Random Forest without hyperparameter tuning\n",
        "rf_model = RandomForestClassifier()\n",
        "rf_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "UB7E7TKr2q5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Deep Learning Model: MLP"
      ],
      "metadata": {
        "id": "PMAoWf5HxTgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple MLP model with dropout for regularization\n",
        "dl_model = keras.Sequential([\n",
        "    keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    keras.layers.Dropout(0.3),\n",
        "    keras.layers.Dense(32, activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')  # For binary classification\n",
        "])"
      ],
      "metadata": {
        "id": "rMmOwUJf2wWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile and train the model\n",
        "dl_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "dl_model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)"
      ],
      "metadata": {
        "id": "O-Yxs_Nl2zXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predicted probabilities from the deep learning model\n",
        "y_prob_dl = dl_model.predict(X_test).flatten()"
      ],
      "metadata": {
        "id": "giWAh_Tn21-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Model Evaluation: Metrics & ROC"
      ],
      "metadata": {
        "id": "QJZjn_R1xZvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate all three models: XGBoost, Random Forest, Deep Learning\n",
        "models = {\n",
        "    'XGBoost': (best_xgb.predict(X_test), best_xgb.predict_proba(X_test)[:, 1]),\n",
        "    'Random Forest': (rf_model.predict(X_test), rf_model.predict_proba(X_test)[:, 1]),\n",
        "    'Deep Learning': ((y_prob_dl > 0.5).astype(int), y_prob_dl)\n",
        "}"
      ],
      "metadata": {
        "id": "KwzLYGvBxas2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through each model and print evaluation metrics\n",
        "for name, (y_pred, y_prob) in models.items():\n",
        "    print(f\"\\n{name} Evaluation:\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "    print(f\"ROC AUC: {roc_auc_score(y_test, y_prob):.2f}\")\n",
        "\n",
        "     # Plot ROC Curve\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc_score(y_test, y_prob):.2f})')\n"
      ],
      "metadata": {
        "id": "kd4JHTV1xe51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sample y_prob from XGBoost:\", models['XGBoost'][1][:10])\n",
        "print(\"Sample y_test:\", y_test[:10])\n"
      ],
      "metadata": {
        "id": "qura_iQ24FBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Clean & Safe ROC Plot for All Models\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plot ROC for each model\n",
        "for name, (y_pred, y_prob) in models.items():\n",
        "    try:\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "        auc = roc_auc_score(y_test, y_prob)\n",
        "        plt.plot(fpr, tpr, label=f\"{name} (AUC = {auc:.2f})\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Skipping {name} due to error: {e}\")\n",
        "\n",
        "# Add random baseline line\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Chance')\n",
        "\n",
        "plt.title(\"ROC Curves\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "y5NUYE634Scj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Ensemble Model: Soft Voting"
      ],
      "metadata": {
        "id": "Vvp74pKXyFID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Average the probabilities from all models and threshold for prediction\n",
        "y_ensemble = (models['XGBoost'][1] + models['Random Forest'][1] + models['Deep Learning'][1]) / 3\n",
        "y_pred_ensemble = (y_ensemble > 0.5).astype(int)"
      ],
      "metadata": {
        "id": "kzNcdzcc3NI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate ensemble performance\n",
        "print(\"\\nEnsemble Model Evaluation:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_ensemble):.2f}\")\n",
        "print(classification_report(y_test, y_pred_ensemble))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_ensemble))\n",
        "print(f\"Ensemble ROC AUC: {roc_auc_score(y_test, y_ensemble):.2f}\")"
      ],
      "metadata": {
        "id": "-zoiZ1E23Qdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Error Analysis"
      ],
      "metadata": {
        "id": "xHcgZvl5yS4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify and count misclassified samples\n",
        "misclassified = X_test[(y_pred_ensemble != y_test)]\n",
        "print(f\"\\nTotal Misclassified Samples: {len(misclassified)}\")"
      ],
      "metadata": {
        "id": "XLkOgRpZySP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. Learning Curve: Random Forest"
      ],
      "metadata": {
        "id": "K7jkIt1SykWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training vs validation accuracy as training size increases\n",
        "train_sizes, train_scores, test_scores = learning_curve(\n",
        "    rf_model, X, y, cv=5, scoring='accuracy',\n",
        "    train_sizes=np.linspace(0.1, 1.0, 5), n_jobs=-1\n",
        ")"
      ],
      "metadata": {
        "id": "Ws-CeBxk3e50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot learning curve\n",
        "train_scores_mean = train_scores.mean(axis=1)\n",
        "test_scores_mean = test_scores.mean(axis=1)\n",
        "\n",
        "plt.plot(train_sizes, train_scores_mean, label='Training score')\n",
        "plt.plot(train_sizes, test_scores_mean, label='Cross-validation score')\n",
        "plt.title(\"Learning Curve - Random Forest\")\n",
        "plt.xlabel(\"Training Size\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FfAOMyHh3ixO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12. Timing Analysis"
      ],
      "metadata": {
        "id": "Xd3PXZNfyxR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Measure time to train and predict using Random Forest\n",
        "start_time = time.time()\n",
        "rf_model.fit(X_train, y_train)\n",
        "train_duration = time.time() - start_time\n",
        "\n",
        "start_time = time.time()\n",
        "_ = rf_model.predict(X_test)\n",
        "inference_duration = time.time() - start_time\n",
        "\n",
        "print(f\"\\nTraining Time (Random Forest): {train_duration:.2f} sec\")\n",
        "print(f\"Inference Time (Random Forest): {inference_duration:.4f} sec\")"
      ],
      "metadata": {
        "id": "9qBiF8K9yyKk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}